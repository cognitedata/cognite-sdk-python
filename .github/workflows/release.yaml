---
name: release

on:
  push:
    branches: [master]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: 3.8

      - name: Install required dependencies
        run: |
          python3 -m pip install --upgrade pip "poetry<1.2.0"
          poetry config virtualenvs.create false
          poetry install -E numpy

      - name: Linting and static code checks
        run: |
          pre-commit run --all-files

  test_core:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: 3.8

      - name: Install required dependencies
        run: |
          python3 -m pip install --upgrade pip "poetry<1.2.0"
          poetry config virtualenvs.create false
          poetry install

      - name: Test core
        env:
          CI: 1
        run: pytest tests/tests_unit -n8 --dist loadscope --maxfail 10 -m 'not dsl'
          --test-deps-only-core

  test_full_and_build_and_release:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: 3.8

      - name: Install full dependencies
        run: |
          python3 -m pip install --upgrade pip "poetry<1.2.0"
          poetry config virtualenvs.create false
          poetry install -E all

      - name: Install protoc compiler
        uses: arduino/setup-protoc@v1

      - name: Verify proto files
        env:
          TMPDIR: latest_proto
          PROTO_URL: https://raw.githubusercontent.com/cognitedata/protobuf-files/master/v1/timeseries
        run: |
          mkdir $TMPDIR
          curl --silent $PROTO_URL/data_points.proto --output $TMPDIR/data_points.proto
          curl --silent $PROTO_URL/data_point_list_response.proto --output $TMPDIR/data_point_list_response.proto
          diff $TMPDIR/data_points.proto cognite/client/_proto/data_points.proto
          diff $TMPDIR/data_point_list_response.proto cognite/client/_proto/data_point_list_response.proto

          protoc --proto_path=$TMPDIR --python_out=$TMPDIR $TMPDIR/*.proto
          sed -i '/import data_points_pb2/c\import cognite.client._proto.data_points_pb2 as data__points__pb2' $TMPDIR/data_point_list_response_pb2.py
          diff $TMPDIR/data_points_pb2.py cognite/client/_proto/data_points_pb2.py
          diff $TMPDIR/data_point_list_response_pb2.py cognite/client/_proto/data_point_list_response_pb2.py

      - name: Test full
        env:
          LOGIN_FLOW: client_credentials
          COGNITE_CLIENT_SECRET: ${{ secrets.COGNITE_CLIENT_SECRET }}
          COGNITE_TOKEN_URL: https://login.microsoftonline.com/dff7763f-e2f5-4ffd-9b8a-4ba4bafba5ea/oauth2/v2.0/token
          COGNITE_TOKEN_SCOPES: https://greenfield.cognitedata.com/.default
          COGNITE_CLIENT_ID: 14fd282e-f77a-457d-add5-928ec2bcbf04
          COGNITE_PROJECT: python-sdk-test
          COGNITE_BASE_URL: https://greenfield.cognitedata.com
          COGNITE_CLIENT_NAME: python-sdk-integration-tests
          CI: 1
        run: |
          pytest tests --cov --cov-report xml:coverage.xml -n8 --dist loadscope --reruns 2

      - uses: codecov/codecov-action@v3
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml

      - name: Build package
        run: poetry build

      - name: Build docs
        run: cd docs && make html

      - name: Release to PyPI
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
        run: twine upload --verbose dist/* || echo 'Version exists'

      - name: Push code snippets to service-contracts
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: sh ./scripts/deploy_code_snippets.sh || echo 'PR failed. There is probably
          nothing to commit'
